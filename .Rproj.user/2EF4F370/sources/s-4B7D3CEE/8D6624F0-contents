---
title: "HomeworkNotes2_week4"
author: "bbneo"
date: "03/27/2015"
output:
  html_document:
    df_print: paged
---

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

# Week 4

## Hypothesis Testing


We distinguish between these two errors:

* A Type I error REJECTS a TRUE null hypothesis $H_0$, and 

* a Type II error ACCEPTS a FALSE null hypothesis $H_0$.


Recall that the standard error of a sample mean is given by
the formula s/sqrt(n). Recall in our sleep example we had a
sample of 100 subjects, our mean RDI (X') was 32 events /
hour with a standard deviation (s) of 10 events / hour. What
is the standard error of the mean in this example?


Under H_0, X' is normally distributed with mean mu=30 and
variance 1. (We're estimating the variance as the square of
the standard error which in this case is 1.) We want to
choose the constant C so that the probability that X is
greater than C given $H_0$ is 5%. That is, P(X > C \$H_0$) is
5%. Sound familiar?

    qnorm(0.95) = `r qnorm(0.95)`

The 95th percentile of a standard normal distribution is
1.645 standard deviations from the mean, so in our example
we have to set C to be 1.645 standard deviations MORE than
our hypothesized mean of 30, that is, C = 30 + 1.645 * 1 =
31.645 (recall that the variance and standard deviation
equalled 1)

How do we do this? Compute the distance between the two means
(32-30) and divide by the standard error of the mean, that is
($s/\sqrt{n}$).

The general rule for rejection is if 

$$Z = \frac{ (X' - \mu) }{ ( s / \sqrt{n} )} > Z_{1-\alpha}$$


## Power

[Power vs. Effect Size: StackOverflow](http://stackoverflow.com/questions/4680163/power-vs-effect-size-plot)

[Genome Wide Association: Wikipedia](http://en.wikipedia.org/wiki/Genome-wide_association_study#Background)


```
power.t.test(n=16,delta=100,sd=200,type="one.sample",alt="one.sided")$power
[1] 0.6040329
```

So keeping the effect size (the ratio delta/sd) constant preserved
the power. Let's try a similar experiment except now we'll specify
a power we want and solve for the sample size n.

First run power.t.test(power = .8, delta = 2 / 4, sd=1, type =
"one.sample", alt = "one.sided")$n .

```
power.t.test(power=.8,delta=2/4,sd=1,type="one.sample", alt = "one.sided")$n
[1] 26.13751
```

Now use power.t.test to find delta for a power=.8 and n=26 and sd=1

```
power.t.test(n=26,power=.8,sd=1,type="one.sample",alt="one.sided")$delta
[1] 0.5013986
```

*Effect size* $$(\Delta/sd) = 0.5, power \thicksim 0.8, 
with n \thicksim 26$$


\pagebreak

## Multiple Testing

Since multiple testing addresses compensating for errors let's
review what we know about them. A Type I error is

Eliminate the two choices that are not errors. A Type I error
involves rejection.

1. failing to reject a false hypothesis
2. failing to reject a true hypothesis
3. rejecting a true hypothesis
4. rejecting a false hypothesis

Selection: 3

In an American court, an example of a Type I error is

1. letting the indicted off on a technicality
2. convicting an innocent person
3. acquitting a guilty person

Selection: 2

You are quite good my friend!


A Type II error is

1. failing to reject a false hypothesis
2. rejecting a true hypothesis
3. failing to reject a true hypothesis
4. rejecting a false hypothesis

Selection: 1

You nailed it! Good job!


In an American court, an example of a Type II error is

1. convicting an innocent person
2. acquitting a guilty person
3. letting the indicted off on a technicality

Selection: 2

That's the answer I was looking for.


Good. Let's continue reviewing. The null hypothesis

1. represents the status_quo and is assumed true
2. is never true
3. is a big nothing that statisticians like to gossip about
4. tells us the origins of the number 0

Selection: 

[Familywise Error Rate: Wikipedia](http://en.wikipedia.org/wiki/Familywise_error_rate)

[Familywise Error Rate: Definition](http://en.wikipedia.org/wiki/Familywise_error_rate#Definitions)

Suppose we've tested m null hypotheses, m\_0 of which are actually true, and m-m\_0
are actually false. Out of the m tests R have been declared
significant, that is, the associated p-values were less than alpha, and m-R were nonsignificant, or boring results.


Looking at the chart, which variables are known?

1. A,B,C
2. m_0, and m
3. S,T,U,V
4. m and R

Selection: 4


In testing the m_0 true null hypotheses, V results were declared
significant, that is, these tests favored the alternative
hypothesis. What type of error does this represent?

1. Type II
2. Type III
3. a serious one
4. Type I

Selection: 4

Excellent job!

Another name for a Type I error is *False Positive*, 
since it is falsely claiming a significant (positive) result.


Of the m-m_0 false null hypotheses, T were declared 
nonsignificant.
This means that these T null hypotheses were accepted (failed to be rejected). What type of error does this represent?

1. Type III
2. a serious one
3. Type II
4. Type I

Selection: 3


Another name for a Type II error is False Negative, since it is
falsely claiming a nonsignificant (negative) result.


Consider the fraction $V/R$.

The observed R represents the number of test results declared
significant. These are 'discoveries', something different from the status quo. V is the number of those falsely declared significant,
so V/R is the ratio of FALSE discoveries. Since V is a random
variable (i.e., unknown until we do an experiment) we call the
expected value of the ratio, E(V/R), the 
**False Discovery Rate (FDR)**.


How about the fraction $V/m_0$? 

From the chart, m\_0 represents the number of true H\_0's and m\_0 is
unknown. V is the number of those falsely declared significant, so
V/m_0 is the ratio of FALSE positives. Since V is a random variable
(i.e., unknown until we do an experiment) we call the expected
value of the ratio, E(V/m_0), the **FALSE POSITIVE rate**.

### Family Wise Error Rate (FWER)


[Familywise Error Rate: Wikipedia](http://en.wikipedia.org/wiki/Familywise_error_rate)

[Familywise Error Rate: Definition](http://en.wikipedia.org/wiki/Familywise_error_rate#Definitions)


We call the probability of at least one false positive, 
$Pr(V >= 1)$ the Family Wise Error Rate (FWER).

So how do we control the False Positive Rate?

Suppose we're really smart, calculate our p-values correctly,
and declare all tests with $p < \alpha$ as significant. 
This means that our false positive rate is at most alpha, on average.

***Bonferroni correction***

We can try to control the family-wise error rate (FWER), the
probability of at least one false positive, with the *Bonferroni correction*, the oldest multiple testing correction.

It's very straightforward. We do m tests and want to control the FWER at level alpha so that $Pr(V >= 1) < \alpha$. We simply reduce
alpha dramatically. Set $\alpha_{fwer} = \alpha/m$. We'll only call a 
test result significant if its $p{-}value < \alpha_{fwer}$.


***Benjamini-Hochberg method (BH)***

Another way to limit the false positive rate is to control the
false discovery rate (FDR). Recall this is E(V/R). This is the most
popular correction when performing lots of tests. It's used in lots
of areas such as genomics, imaging, astronomy, and other
signal-processing disciplines.

Again, we'll do m tests but now we'll set the FDR, or E(V/R) at
level alpha. We'll calculate the p-values as usual and order them
from smallest to largest, $p_1, p_2,...p_m$. We'll call significant
any result with $p_i <= (\alpha*i)/m$. 
This is the Benjamini-Hochberg
method (BH). A p-value is compared to a value that depends on its ranking.


***p-value Correction***

Now consider this chart copied from the slides. It shows the
p-values for 10 tests performed at the alpha=.2 level and three
cutoff lines. The p-values are shown in order from left to right
along the x-axis. The red line is the threshold for No Corrections
(p-values are compared to alpha=.2), the blue line is the
Bonferroni threshold, alpha=.2/10 = .02, and the gray line shows
the BH correction. Note that it is not horizontal but has a
positive slope as we expect.

How many points fall below the red line?

1. 4
2. 6
3. 2
4. 8

Selection: 1

All that hard work is paying off!

With the Bonferroni correction, how many tests are declared
significant?

1. 8
2. 6
3. 2
4. 4

Selection: 3

That's correct!

So the Bonferroni passed only half the results that the No
Correction (comparing p-values to alpha) method passed. Now look at
the BH correction. How many tests are significant with this scale?

1. 3
2. 5
3. 7
4. 1

Selection: 1

You are doing so well!


So the BH correction which limits the FWER is between the No
Correction and the Bonferroni. It's more conservative (fewer
significant results) than the No Correction but less conservative
(more significant results) than the Bonferroni. Note that with this
method the threshold is proportional to the ranking of the values
so it slopes positively while the other two thresholds are flat.


Notice how both the Bonferroni and BH methods adjusted the
threshold (alpha) level of rejecting the null hypotheses. Another
equivalent corrective approach is to adjust the p-values, so
they're not classical p-values anymore, but they can be compared
directly to the original alpha.


Suppose the p-values are $p_1, ... , p_m$.  With the Bonferroni
method you would adjust these by setting $p'_i = max(m * p_i, 1)$ for
each p-value. Then if you call all $p'_i < \alpha$ significant you
will control the FWER.


To demonstrate some of these concepts, we've created an array of
p-values for you. It is 1000-long and the result of a linear
regression performed on random normal x,y pairs so there is no true
significant relationship between the x's and y's.


Use the R command head to see the first few entries of the array
pValues.

```
> head(pValues)
[1] 0.5334915 0.2765785 0.8380943 0.6721730 0.8122037 0.4078675
```

Great job!



Now count the number of entries in the array that are less than the
value .05. Use the R command sum, and the appropriate Boolean
expression.

```
> sum(pValues < .05)
[1] 51
```

Your dedication is inspiring!


So we got around 50 false positives, just as we expected
(.05*1000=50). The beauty of R is that it provides a lot of
built-in statistical functionality. The function p.adjust is one
example. The first argument is the array of pValues. Another
argument is the method of adjustment. Once again, use the R
function sum and a boolean expression using p.adjust with
method="bonferroni" to control the FWER.

```
> sum(p.adjust(pValues,method="bonferroni") < .05)
[1] 0
```

You are doing so well!


So the correction eliminated all the false positives that had
passed the uncorrected alpha test. Repeat the same experiment, this
time using the method "BH" to control the FDR.

```
> sum(p.adjust(pValues,method="BH") < .05)
[1] 0
```

All that hard work is paying off!


### Finding Significance

So the BH method also eliminated all the false positives. Now we've
generated another 1000-long array of p-values, this one called
pValues2. In this data, the first half ( 500 x/y pairs) contains x
and y values that are random and the second half contain x and y
pairs that are related, so running a linear regression model on the
1000 pairs should find some significant (not random) relationship.

We also created a 1000-long array of character strings, trueStatus.
The first 500 entries are "zero" and the last are "not zero". Use
the R function tail to look at the end of trueStatus.

```
> tail(trueStatus)
[1] "not zero" "not zero" "not zero" "not zero" "not zero" "not zero"
```

Great job!


Once again we can use R's greatness to count and tabulate for us.
We can call the R function table with two arguments, a boolean such
as pValues2<.05, and the array trueStatus. The boolean obviously
has two outcomes and each entry of trueStatus has one of two
possible values. The function table aligns the two arguments and
counts how many of each combination (TRUE,"zero"), (TRUE,"not
zero"), (FALSE,"zero"), and (FALSE,"not zero") appear. Try it now.

```
> table(pValues2<.05, trueStatus)
       trueStatus
        not zero zero
  FALSE        0  476
  TRUE       500   24
```

Your dedication is inspiring!


We see that without any correction all 500 of the truly significant
(nonrandom) tests were correctly identified in the "not zero"
column. In the zero column (the truly random tests), however, 24
results were flagged as significant.


What is the percentage of false positives in this test?

```
> 24/500
[1] 0.048
```

You are doing so well!


Just as we expected - around 5% or .05*1000.


Now run the same table function, however, this time use the call to
p.adjust with the "bonferroni" method in the boolean expression.
This will control the FWER.

```
> table(p.adjust(pValues2,method="bonferroni")<.05, trueStatus)
       trueStatus
        not zero zero
  FALSE       23  500
  TRUE       477    0
```

All that hard work is paying off!

Since the Bonferroni correction method is more conservative than
just comparing p-values to alpha all the truly random tests are
correctly identified in the zero column. In other words, we have no
false positives. However, the threshold has been adjusted so much
that 23 of the truly significant results have been misidentified in
the not zero column.


Now run the same table function one final time. Use the call to
p.adjust with "BH" method in the boolean expression. This will
control the false discovery rate.

```
> table(p.adjust(pValues2,method="BH")<.05, trueStatus)
       trueStatus
        not zero zero
  FALSE        0  487
  TRUE       500   13
```

Your dedication is inspiring!


Again, the results are a compromise between the No Corrections and
the Bonferroni. All the significant results were correctly
identified in the "not zero" column but in the random ("zero")
column 13 results were incorrectly identified. These are the false
positives. This is roughly half the number of errors in the other
two runs.


Here's a plot of the two sets of adjusted p-values, Bonferroni on
the left and BH on the right. The x-axis indicates the original
p-values. For the Bonferroni, (adjusting by multiplying by 1000,
the number of tests), only a few of the adjusted values are below
1. For the BH, the adjusted values are slightly larger than the
original values.


We'll conclude by saying that multiple testing is an entire
subfield of statistical inference. Usually a basic Bonferroni/BH
correction is good enough to eliminate false positives, but if
there is strong dependence between tests there may be problems.
Another correction method to consider is "BY".

\pagebreak

## Resampling

### Bootstrapping

It's relatively new, developed in 1979, by Bradley Efron, a
Stanford statistician.  The basic bootstrap principle uses OBSERVED
data to construct an ESTIMATED population distribution using random
sampling with replacement. From this distribution (constructed from
the observed data) we can estimate the distribution of the
statistic we're interested in.

***Example of Bootstrapping: Fathers Heights***

We've created the vector fh for you which contains the fathers'
heights from the father son data we've been working with. It's the
same length as the sons' data (1078) which is stored in nh. B, the
number of bootstraps we want has been set to 1000. We'll do an
example now in small steps.


Our one sample of observed data is in the vector fh. Use the R
function sample to sample fh nh*B times. Set the argument replace
to TRUE. Put the result in the variable sam.

```
> sam <- sample(fh, nh*B, replace=TRUE)
```

All that hard work is paying off!

Now form sam into a matrix with B rows and nh columns. Use the R
function matrix and put the result in resam

```
> resam <- matrix(sam,B,nh)
```

That's a job well done!

Now use the R function apply to take the median (third argument) of
each row of resam (first argument). Put the result in meds. The
second argument, the number 1, specifies that the application of
the function is to the rows of the first argument.

```
> meds <- apply(resam, 1, median)
```

Keep up the great work!

Now look at the difference between the median of fh and the median
of meds.

```
> median(fh) - median(meds)
[1] 0
```

Excellent job!

Pretty close, right? Now use the R function sd to estimate the
standard error of the vector meds.

```
> sd(meds)
[1] 0.1029482
```

Keep up the great work!

We previously did this same process for the sons' data and stored
the resampled medians in the 1000-long vector resampledMedians.
Find the standard error of resampledMedians.

```
> sd(resampledMedians)
[1] 0.08305161
```

Perseverance, that's the answer.

Now we'll find a 95% confidence interval for the sons' data with
the R function quantile. The first argument is the vector of
resampledMedians and the second is the expression c(.025,.975). Do this now.

```
> quantile(resampledMedians,c(.025,.975))
    2.5%    97.5% 
68.43733 68.81061 
```

Nice work!

Pretty close quantiles, right? Now do the same thing for the
fathers' data. Recall that it's stored in the vector meds.

```
> quantile(meds,c(.025,.975))
    2.5%    97.5% 
67.55466 67.94197 
```

You got it!

Another pair of close quantiles, but notice that these quantiles of
the fathers' medians differ from those of the sons.


### Permutation Testing

Now, to permutation testing, another handy tool used in group
comparisons. As bootstrapping did, permutation testing samples a
single dataset a zillion times and calculates a statistic based on
these samplings.

Permutation testing, however, is based on the idea of
exchangability of group labels. It measures whether or not outcomes
are independent of group identity. Our zillion samples simply
permute group labels associated with outcomes. We'll see an example of this.

![Insect Sprays](InsectSprays.png)


We'll use permutation testing to compare Spray B with Spray C.

Use the R command dim to find the dimensions of InsectSprays.

```
> dim(InsectSprays)
[1] 72  2
```

You are quite good my friend!

Now use the R command names to find what the two columns of
InsectSprays contain.

```
> names(InsectSprays)
[1] "count" "spray"
```

Excellent work!

We'll use permutation testing to compare Spray B with Spray C. We
subsetted data for these two sprays into a data frame subdata.
Moreover, the two data frames Bdata and Cdata contain the data for
their respective sprays.

Now use the R command range on Bdata$count to find the minimum and
maximum counts for Spray B.

```{r,eval=FALSE}
> range(Bdata$count)
[1]  7 21
```

You got it!

The picture makes more sense now, right? Now do the same for Spray C.
Its data is in Cdata.

```{r,eval=FALSE}
> range(Cdata$count)
[1] 0 7
```

Excellent work!

From the ranges (as well as the picture), the sprays look a lot
different. We'll test the (obviously false) null hypothesis that
their means are the same.

To make the analysis easier we've defined two arrays for you, one
holding the counts for sprays B and C. It's call BCcounts. Look at it
now.

```{r,eval=FALSE}
> BCcounts
 [1] 11 17 21 11 16 14 17 17 19 21  7 13  0  1  7  2  3  1  2  1  3  0
[23]  1  4
```

You are doing so well!

The second array we've defined holds the spray identification and
it's called group. These two arrays line up with each other, that is,
the first 12 entries of counts are associated with spray B and the
last 12 with spray C.  Look at group now.

```{r,eval=FALSE}
> group
 [1] "B" "B" "B" "B" "B" "B" "B" "B" "B" "B" "B" "B" "C" "C" "C" "C" "C"
[18] "C" "C" "C" "C" "C" "C" "C"
```

That's correct!

We've also defined for you a one-line function testStat which takes
two parameters, an array of counts and an array of associated
identifiers. It assumes all the counts come from group B or group C.
It subtracts the mean of the counts from group C from the mean of the
counts of group B. Type testStat with no parentheses and no arguments
to see how it's defined.

```{r,eval=FALSE}
> testStat
function(w, g) mean(w[g == "B"]) - mean(w[g == "C"])
```

All that practice is paying off!

Now set a variable obs by invoking testStat with the arguments
BCcounts and group and assigning the result to obs.

```{r,eval=FALSE}
> obs <- testStat(BCcounts,group)
```

You got it!

Take a peek at obs now.

```{r,eval=FALSE}
> obs
[1] 13.25
```

You nailed it! Good job!

Pretty big difference, right? You can check this by using mean on
Bdata$count and on Cdata$count and subtracting the latter from the
former. Equivalently, you can just apply mean to
Bdata$count-Cdata$count. Do either one now.

```{r,eval=FALSE}
> mean(Bdata$count-Cdata$count)
[1] 13.25
```

That's the answer I was looking for.

So, `mean(Bdata$count)-mean(Cdata$count)` equals
`mean(Bdata$count-Cdata$count)` because ?

1: mathemagic
2: the data is special
3: mean is linear

Selection: 3

Great job!

***Resampling/Permuting the group labels***

Now this is where the permutation testing starts to involve
resampling. We're going to test whether or not the particular group
association of the counts affects the difference of the means.

We'll keep the same array of counts, just randomly relabel them, by
permuting the group array. R makes this process very easy. Calling
the function sample (which we've used several times in this lesson)
with one argument, an array name, will simply permute the elements of
that array.

Call sample now on the array group to see what happens.

```
> sample(group)
 [1] "C" "C" "C" "C" "B" "C" "B" "C" "C" "C" "B" "C" "B" "B" "B" "B" "C"
[18] "B" "B" "C" "B" "B" "B" "C"
```

That's a job well done!

The labels are all mixed up now. We'll do this permuting of labels
and then we'll recalculate the difference of the means of the two
"new" (really newly labelled) groups.

We'll relabel and calculate the difference of means 10000 times and
store the differences (of means) in the array perms. Here's what the
code looks like perms <- sapply(1 : 10000, function(i)
testStat(BCcounts, sample(group))). Try it now.


> perms <- sapply(1:10000, function(i) testStat(BCcounts,sample(group)))

Perseverance, that's the answer.


We can take the mean of the virtual array of the boolean expression
perms > obs. Do this now.

> 
> mean(perms > obs)
[1] 0

You nailed it! Good job!

So on average 0 of the permutations had a difference greater than the
observed. That means we would reject the null hypothesis that the
means of the two sprays were equal.


Here's a histogram of the difference of the means. Looks pretty
normal, right? We can see that the distribution runs roughly between
-10 and +10 and it's centered around 0. The vertical line shows where
the observed difference of means was and we see that it's pretty far
away from the distribution of the resampled permutations. This means
that group identification did matter and sprays B and C were quite
different.


![Insect Sprays Permuted Means Distr](InsectSpr_PermutMeans.png)


Here's the picture of the InsectSprays again. Suppose we run the same
experiment, this time comparing sprays D and E, which look more
alike. We've redefined testStat to look at these sprays and subtract
the mean of spray E from the mean of spray D.

We've also stored off the D and E data in DEcounts and the group
labels in group. Run testStat now with DEcounts and group.

```{r,eval=FALSE}
> testStat(DEcounts,group)
[1] 1.416667
```

That's a job well done!

We've stored off this value, 1.416667, in the variable obs for you.
Now run the permutation command, with DEcounts. Here it is, perms <-
sapply(1 : 10000, function(i) testStat(DEcounts, sample(group)))

```{r,eval=FALSE}
> perms <- sapply(1:10000, function(i) testStat(DEcounts, sample(group)))
```

You are amazing!

Finally, we can plot the histogram of the distribution of the
difference of the means. We see that with these sprays the observed
difference of means (the vertical line) is closer to the mean of the
permuted labels. This indicates that sprays D and E are quite similar
and we fail to reject the null hypothesis that the means were equal.

![Insect Sprays DE Permuted Means Distr](InsectSprDE_histogram.png)

\pagebreak

## Homework \#4


7. A web site was monitored for a year and it received 520 hits per day. In the first 30 days in the next year, the site received 15,800 hits. Assuming that web hits are Poisson.

    Give an exact one sided P-value to the hypothesis that web hits are up this year over last to four significant digits (expressed as a proportion).
    
```{r,eval=FALSE}
# Consider using ppois with lambda=520*30. Note this is nearly exactly Gaussian, so one could get away with the Gaussian calculation.
# 
# This test comes with the important assumption that web hits are a Poisson process.

pv <- ppois(15800 - 1, lambda = 520 * 30, lower.tail = FALSE)

# The answer to 1 is 0.0553
# The answer to 2 is 0
# 
# Also, compare with the Gaussian approximation where lambda ~ N(lambda,lambda/t)

pnorm(15800 / 30, mean = 520, sd = sqrt(520 / 30), lower.tail = FALSE)

[1] 0.05466
```

8. Suppose that in an AB test, one advertising scheme led to an average of 10 purchases per day for a sample of 100 days, while the other led to 11 purchaces per day, also for a sample of 100 days. Assuming a common standard deviation of 4 purchases per day. Assuming that the groups are independent and that they days are iid, perform a Z test of equivalence.

    What is the P-value reported to 3 digits expressed as a proportion?

```{r,eval=FALSE}
m1 <- 10; m2 <- 11
n1 <- n2 <- 100
s <- 4
se <- s * sqrt(1 / n1 + 1 / n2)
ts <- (m2 - m1) / se
pv <- 2 * pnorm(-abs(ts))
```

13. Consider the mtcars data set.

    Give the p-value for a t-test comparing MPG for 6 and 8 cylinder cars assuming equal variance, as a proportion to 3 decimal places.
    Give the associated P-value for a z test.
    Give the common standard deviation estimate for MPG across cylinders to 3 decimal places.
    Would the t test reject at the two sided 0.05 level (0 for no 1 for yes)?
    
```{r,eval=FALSE}
mpg8 <- mtcars$mpg[mtcars$cyl == 8]
mpg6 <- mtcars$mpg[mtcars$cyl == 6]
m8 <- mean(mpg8); m6 <- mean(mpg6)
s8 <- sd(mpg8); s6 <- sd(mpg6)
n8 <- length(mpg8); n6 <- length(mpg6)
```

```{r,eval=FALSE}
p <- t.test(mpg8, mpg6, paired = FALSE, alternative="two.sided", var.equal=TRUE)$p.value
mixprob <- (n8 - 1) / (n8 + n6 - 2)
s <- sqrt(mixprob * s8 ^ 2  +  (1 - mixprob) * s6 ^ 2)
z <- (m8 - m6) / (s * sqrt(1 / n8 + 1 / n6))
pz <- 2 * pnorm(-abs(z))

## Hand calculating the T just to check
#2 * pt(-abs(z), df = n8 + n6 - 2)

#    0
#    0
#    2.27
#    1
```

